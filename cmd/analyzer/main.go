package main

import (
	"bytes"
	"encoding/base64"
	"encoding/json"
	"flag"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"path/filepath"
	"strings"
	"time"

	"log-analyzer/internal/aggregator"
	"log-analyzer/internal/config"
	"log-analyzer/internal/normalizer"
	"log-analyzer/internal/preprocessor"
	"log-analyzer/internal/reporter"
	"log-analyzer/pkg/models"
)

func main() {
	// OpenSearch parameters
	fetchFromOpenSearch := flag.Bool("fetch", false, "Fetch logs from OpenSearch instead of using saved files")
	timeRange := flag.String("time", "1h", "Time range for OpenSearch query (e.g., '1h', '24h')")
	keyword := flag.String("keyword", "error", "Search keyword for OpenSearch query")
	indices := flag.String("indices", "pp-slot-api-log*", "OpenSearch indices to query (comma-separated)")
	windowSize := flag.String("window", "30m", "Time window size for fetching (default: 30 minutes)")

	// File parameters
	inputDir := flag.String("input", "./data/opensearch-responses", "Directory containing saved OpenSearch JSON files")
	outputDir := flag.String("output", "./reports", "Directory to save generated reports")
	flag.Parse()

	fmt.Println("ğŸš€ å•Ÿå‹•æ—¥èªŒåˆ†æç®¡é“ï¼ˆæ”¯æ´æ™‚é–“çª—å£ï¼‰")
	fmt.Println()

	// Step 0: Fetch from OpenSearch if requested
	var rawLogs []models.RawLog
	var err error

	if *fetchFromOpenSearch {
		fmt.Printf("ğŸ“¡ ç¬¬ 0 æ­¥ï¼šå¾ OpenSearch ç²å–æ—¥èªŒï¼ˆéå» %sï¼Œçª—å£å¤§å°ï¼š%sï¼‰...\n", *timeRange, *windowSize)
		rawLogs, err = fetchFromOpenSearchWithWindows(*timeRange, *keyword, *indices, *windowSize)
		if err != nil {
			log.Fatalf("ç„¡æ³•å¾ OpenSearch ç²å–ï¼š%v", err)
		}
		fmt.Printf("âœ… å¾å¤šå€‹æ™‚é–“çª—å£æˆåŠŸç²å– %d æ¢æ—¥èªŒ\n\n", len(rawLogs))

		// If fetch mode but no data, show warning and exit (don't use mock)
		if len(rawLogs) == 0 {
			fmt.Println("âš ï¸  æŒ‡å®šæ™‚é–“ç¯„åœå…§ OpenSearch ä¸­æ‰¾ä¸åˆ°æ—¥èªŒã€‚")
			fmt.Println("   æç¤ºï¼šå˜—è©¦æ›´é•·çš„æ™‚é–“ç¯„åœæˆ–ä¸åŒçš„æœå°‹é—œéµå­—")
			fmt.Println("   ç¯„ä¾‹ï¼š-time 48h -keyword warning")
			os.Exit(0)
		}
	} else {
		// Step 1: Load raw logs from JSON
		fmt.Println("ğŸ“¥ ç¬¬ 1 æ­¥ï¼šå¾ JSON æª”æ¡ˆåŠ è¼‰åŸå§‹æ—¥èªŒ...")
		rawLogs, err = loadRawLogsFromJSON(*inputDir)
		if err != nil {
			log.Fatalf("ç„¡æ³•åŠ è¼‰åŸå§‹æ—¥èªŒï¼š%v", err)
		}
		fmt.Printf("âœ… æˆåŠŸåŠ è¼‰ %d æ¢åŸå§‹æ—¥èªŒ\n\n", len(rawLogs))

		// If file mode and no data, offer to use mock for demonstration
		if len(rawLogs) == 0 {
			fmt.Println("âš ï¸  ç›®éŒ„ä¸­æ‰¾ä¸åˆ°æ—¥èªŒã€‚å»ºç«‹ç¤ºç¯„ç”¨çš„æ¨¡æ“¬æ•¸æ“š...")
			rawLogs = createMockData()
			fmt.Printf("âœ… å·²å»ºç«‹ %d æ¢æ¨¡æ“¬æ—¥èªŒç”¨æ–¼æ¸¬è©¦\n\n", len(rawLogs))
		}
	}

	// Step 2: Preprocess logs
	fmt.Println("ğŸ”„ ç¬¬ 2 æ­¥ï¼šé è™•ç†æ—¥èªŒ...")
	preprocessor := preprocessor.NewLogPreprocessor()
	parsedLogs, err := preprocessor.Process(rawLogs)
	if err != nil {
		log.Fatalf("ç„¡æ³•é è™•ç†æ—¥èªŒï¼š%v", err)
	}
	fmt.Printf("âœ… æˆåŠŸè§£æ %d æ¢æ—¥èªŒ\n\n", len(parsedLogs))

	// Step 3: Normalize and group by fingerprint
	fmt.Println("ğŸ” ç¬¬ 3 æ­¥ï¼šæ­£è¦åŒ–å’Œåˆ†çµ„éŒ¯èª¤...")
	norm := normalizer.NewLogNormalizer()
	errorGroups, err := norm.Normalize(parsedLogs)
	if err != nil {
		log.Fatalf("ç„¡æ³•æ­£è¦åŒ–æ—¥èªŒï¼š%v", err)
	}
	normStats := normalizer.GetNormalizationStats(len(parsedLogs), errorGroups)
	fmt.Printf("âœ… åˆ†çµ„ç‚º %d å€‹å”¯ä¸€éŒ¯èª¤æ¨¡å¼ï¼ˆ%.1f%% é‡è¤‡ç‡ï¼‰\n\n",
		len(errorGroups), normStats.DuplicationRate*100)

	// Step 4: Aggregate statistics
	fmt.Println("ğŸ“Š ç¬¬ 4 æ­¥ï¼šèšåˆçµ±è¨ˆè³‡è¨Š...")
	agg := aggregator.NewLogAggregator()
	aggResult, err := agg.Aggregate(errorGroups)
	if err != nil {
		log.Fatalf("ç„¡æ³•èšåˆï¼š%v", err)
	}
	aggStats := aggregator.GetAggregationStats(aggResult)
	fmt.Printf("âœ… èšåˆå®Œæˆï¼š\n")
	fmt.Printf("   - ç¸½éŒ¯èª¤æ•¸ï¼š%d\n", aggStats.TotalLogs)
	fmt.Printf("   - æœå‹™ç¸½æ•¸ï¼š%d\n", aggStats.TotalServices)
	fmt.Printf("   - å³°å€¼æ™‚æ®µï¼š%02d:00ï¼ˆ%d å€‹éŒ¯èª¤ï¼‰\n", aggStats.PeakHour, aggStats.PeakCount)
	fmt.Printf("   - å¹³å‡å¯†åº¦ï¼š%.2f éŒ¯èª¤/åˆ†é˜\n\n", aggStats.AverageDensity)

	// Step 5: Generate analyses from actual error groups
	fmt.Println("ğŸ” ç¬¬ 5 æ­¥ï¼šåˆ†æéŒ¯èª¤æ¨¡å¼...")
	analyses := createAnalysesFromErrorGroups(errorGroups)
	fmt.Printf("âœ… å¾å¯¦éš›æ•¸æ“šå»ºç«‹äº† %d å€‹åˆ†æçµæœ\n\n", len(analyses))

	// Step 6: Generate report
	fmt.Println("ğŸ“„ ç¬¬ 6 æ­¥ï¼šç”Ÿæˆ Markdown å ±å‘Š...")
	rep := reporter.NewMarkdownReporter(*outputDir)
	report, err := rep.Generate(analyses, aggResult)
	if err != nil {
		log.Fatalf("ç„¡æ³•ç”Ÿæˆå ±å‘Šï¼š%v", err)
	}
	fmt.Printf("âœ… å ±å‘Šå·²ç”Ÿæˆï¼š%s\n\n", report.ReportPath)

	// Step 7: Save analysis JSON
	fmt.Println("ğŸ’¾ ç¬¬ 7 æ­¥ï¼šå°‡åˆ†æçµæœä¿å­˜ç‚º JSON...")
	if err := reporter.SaveAnalysisJSON(analyses, aggResult, *outputDir); err != nil {
		log.Fatalf("ç„¡æ³•ä¿å­˜åˆ†æ JSONï¼š%v", err)
	}
	fmt.Println("âœ… åˆ†æ JSON å·²ä¿å­˜")

	// Summary
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("âœ¨ å®Œæ•´ç®¡é“æ¸¬è©¦æˆåŠŸå®Œæˆï¼")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("\nğŸ“Š æœ€çµ‚çµ±è¨ˆè³‡è¨Šï¼š\n")
	fmt.Printf("   è¼¸å…¥æ—¥èªŒæ•¸ï¼š%d\n", len(rawLogs))
	fmt.Printf("   è§£ææ—¥èªŒæ•¸ï¼š%d\n", len(parsedLogs))
	fmt.Printf("   éŒ¯èª¤ç¾¤çµ„æ•¸ï¼š%d\n", len(errorGroups))
	fmt.Printf("   å—å½±éŸ¿æœå‹™æ•¸ï¼š%d\n", len(aggResult.ServiceStats))
	fmt.Printf("   è™•ç†æ™‚é–“ï¼š%dms\n\n", aggResult.ProcessingTime.Milliseconds())

	fmt.Printf("ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š\n")
	fmt.Printf("   å ±å‘Šï¼š%s\n", report.ReportPath)
	fmt.Printf("   åˆ†æ JSONï¼š%s/analysis_*.json\n\n", *outputDir)

	fmt.Println("âœ… æ‚¨ç¾åœ¨å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å ±å‘Šå’Œåˆ†æ JSON æª”æ¡ˆï¼")
}

// fetchFromOpenSearchWithWindows fetches logs with time window splitting
func fetchFromOpenSearchWithWindows(timeRangeStr, keyword, indicesStr, windowSizeStr string) ([]models.RawLog, error) {
	// Parse time range and window size
	duration, err := time.ParseDuration(timeRangeStr)
	if err != nil {
		return nil, fmt.Errorf("invalid time range: %w", err)
	}

	windowDuration, err := time.ParseDuration(windowSizeStr)
	if err != nil {
		return nil, fmt.Errorf("invalid window size: %w", err)
	}

	endTime := time.Now()

	// Calculate number of windows
	numWindows := int(duration / windowDuration)
	if numWindows == 0 {
		numWindows = 1
	}

	fmt.Printf("   ğŸ“Š Fetching logs across %d time windows (%.0f minutes each)\n", numWindows, windowDuration.Minutes())
	fmt.Println()

	var allLogs []models.RawLog

	// Fetch data for each window
	for i := 0; i < numWindows; i++ {
		windowEnd := endTime.Add(-time.Duration(i) * windowDuration)
		windowStart := windowEnd.Add(-windowDuration)

		fmt.Printf("   ğŸ• Window %d/%d: %s to %s... ", i+1, numWindows,
			windowStart.Format("15:04:05"), windowEnd.Format("15:04:05"))

		logs, err := fetchFromOpenSearchDashboards(windowStart, windowEnd, keyword, indicesStr)
		if err != nil {
			fmt.Printf("âŒ Error: %v\n", err)
			continue
		}

		fmt.Printf("âœ… %d logs\n", len(logs))
		allLogs = append(allLogs, logs...)
	}

	fmt.Println()
	return allLogs, nil
}

// fetchFromOpenSearchDashboards fetches logs from a specific time window
func fetchFromOpenSearchDashboards(startTime, endTime time.Time, keyword, indicesStr string) ([]models.RawLog, error) {
	// Load config
	cfg, err := config.Load("./configs/config.yaml")
	if err != nil {
		return nil, fmt.Errorf("failed to load config: %w", err)
	}

	// Build query
	query := buildDashboardsQuery(startTime, endTime, keyword)

	// Make request
	client := &http.Client{Timeout: 30 * time.Second}

	indices := strings.Split(indicesStr, ",")
	if len(indices) == 0 {
		indices = []string{"pp-slot-api-log*"}
	}

	var allLogs []models.RawLog

	for _, index := range indices {
		index = strings.TrimSpace(index)

		body := map[string]interface{}{
			"params": map[string]interface{}{
				"index": index,
				"body":  query,
			},
		}

		jsonBody, err := json.Marshal(body)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal request: %w", err)
		}

		url := fmt.Sprintf("%s/internal/search/opensearch-with-long-numerals", cfg.OpenSearch.URL)
		req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonBody))
		if err != nil {
			return nil, fmt.Errorf("failed to create request: %w", err)
		}

		// Set headers
		req.Header.Set("Content-Type", "application/json")
		req.Header.Set("Authorization", "Basic "+basicAuth(cfg.OpenSearch.Username, cfg.OpenSearch.Password))
		req.Header.Set("osd-xsrf", "osd-fetch")
		req.Header.Set("osd-version", "3.0.0")

		resp, err := client.Do(req)
		if err != nil {
			return nil, fmt.Errorf("failed to fetch from OpenSearch: %w", err)
		}
		defer resp.Body.Close()

		if resp.StatusCode != http.StatusOK {
			body, _ := io.ReadAll(resp.Body)
			return nil, fmt.Errorf("OpenSearch API returned %d: %s", resp.StatusCode, string(body))
		}

		// Parse response
		var response map[string]interface{}
		if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
			return nil, fmt.Errorf("failed to decode response: %w", err)
		}

		// Extract hits
		var hitsArray []interface{}
		if rawResp, ok := response["rawResponse"].(map[string]interface{}); ok {
			if hits, ok := rawResp["hits"].(map[string]interface{}); ok {
				if hits, ok := hits["hits"].([]interface{}); ok {
					hitsArray = hits
				}
			}
		}

		// Process hits
		for _, hit := range hitsArray {
			hitMap := hit.(map[string]interface{})

			source, ok := hitMap["_source"].(map[string]interface{})
			if !ok {
				continue
			}

			sourceBytes, _ := json.Marshal(source)
			var openSearchSource models.OpenSearchSource
			if err := json.Unmarshal(sourceBytes, &openSearchSource); err != nil {
				continue
			}

			rawLog := models.RawLog{
				Index:  index,
				ID:     hitMap["_id"].(string),
				Source: openSearchSource,
			}

			allLogs = append(allLogs, rawLog)
		}
	}

	return allLogs, nil
}

// buildDashboardsQuery builds a query for specific time window
func buildDashboardsQuery(startTime, endTime time.Time, keyword string) map[string]interface{} {
	return map[string]interface{}{
		"sort": []map[string]interface{}{
			{
				"@timestamp": map[string]interface{}{
					"order":         "desc",
					"unmapped_type": "boolean",
				},
			},
		},
		"size": 500,
		"_source": map[string]interface{}{
			"excludes": []string{},
		},
		"query": map[string]interface{}{
			"bool": map[string]interface{}{
				"must": []interface{}{},
				"filter": []interface{}{
					map[string]interface{}{
						"multi_match": map[string]interface{}{
							"type":    "phrase",
							"query":   keyword,
							"lenient": true,
						},
					},
					map[string]interface{}{
						"range": map[string]interface{}{
							"@timestamp": map[string]interface{}{
								"gte":    startTime.Format(time.RFC3339),
								"lte":    endTime.Format(time.RFC3339),
								"format": "strict_date_optional_time",
							},
						},
					},
				},
				"should":   []interface{}{},
				"must_not": []interface{}{},
			},
		},
	}
}

// basicAuth creates a basic auth header value
func basicAuth(username, password string) string {
	credentials := fmt.Sprintf("%s:%s", username, password)
	return base64.StdEncoding.EncodeToString([]byte(credentials))
}

// loadRawLogsFromJSON loads raw logs from saved JSON files
func loadRawLogsFromJSON(inputDir string) ([]models.RawLog, error) {
	var allLogs []models.RawLog

	files, err := filepath.Glob(filepath.Join(inputDir, "all-documents_*.json"))
	if err != nil {
		return nil, err
	}

	for _, file := range files {
		data, err := os.ReadFile(file)
		if err != nil {
			fmt.Printf("âš ï¸  Warning: Failed to read file %s: %v\n", file, err)
			continue
		}

		var response map[string]interface{}
		if err := json.Unmarshal(data, &response); err != nil {
			fmt.Printf("âš ï¸  Warning: Failed to unmarshal file %s: %v\n", file, err)
			continue
		}

		if docs, ok := response["documents"].([]interface{}); ok {
			for _, doc := range docs {
				docBytes, _ := json.Marshal(doc)
				var rawLog models.RawLog
				if err := json.Unmarshal(docBytes, &rawLog); err != nil {
					continue
				}
				allLogs = append(allLogs, rawLog)
			}
		}
	}

	return allLogs, nil
}

// createMockData creates mock log data for testing
func createMockData() []models.RawLog {
	now := time.Now()

	t1 := now.Add(-3 * time.Hour).Truncate(time.Hour).Add(30*time.Minute + 45*time.Second)

	mockLogs := []models.RawLog{
		{
			Index: "pp-slot-api-log*",
			ID:    "mock-1",
			Source: models.OpenSearchSource{
				Message: fmt.Sprintf(`%s stderr F {"@timestamp":"%s","caller":"api/handler.go:123","content":"Connection timeout","level":"error","span":"span-123","trace":"trace-456","servicename":"pp-slot-api"}`,
					t1.Format("2006-01-02T15:04:05.000Z"), t1.Format(time.RFC3339)),
				Fields: models.FieldsData{
					ServiceName: "pp-slot-api",
				},
				Timestamp: t1,
			},
		},
	}

	return mockLogs
}

// createAnalysesFromErrorGroups creates analysis results from actual error groups
func createAnalysesFromErrorGroups(groups []models.ErrorGroup) []models.Analysis {
	var analyses []models.Analysis
	registry := config.GetRegistry()

	for _, group := range groups {
		severity := models.SeverityLow
		if group.TotalCount >= 50 {
			severity = models.SeverityHigh
		} else if group.TotalCount >= 10 {
			severity = models.SeverityMedium
		}

		// Try to match against known issues
		var isKnown bool
		var issueID string
		matchedIssue := registry.MatchContentAndService(group.NormalizedContent, group.ServiceName)
		if matchedIssue != nil {
			isKnown = true
			issueID = matchedIssue.ID
		}

		analysis := models.Analysis{
			ErrorGroupID: group.Fingerprint[:8],
			IsKnown:      isKnown,
			Severity:     severity,
			Reason:       fmt.Sprintf("éŒ¯èª¤åœ¨æœå‹™ %s ä¸­ç™¼ç”Ÿäº† %d æ¬¡", group.ServiceName, group.TotalCount),
			SuggestedActions: []string{
				fmt.Sprintf("èª¿æŸ¥éŒ¯èª¤æ¨¡å¼ï¼š%s", truncateString(group.NormalizedContent, 60)),
				fmt.Sprintf("æª¢æŸ¥ä¾†è‡ªèª¿ç”¨è€…çš„æ—¥èªŒï¼š%s", group.CallerFile),
				fmt.Sprintf("èˆ‡éƒ¨ç½²æˆ–é…ç½®è®Šæ›´ç›¸é—œè¯"),
			},
		}

		if isKnown {
			analysis.IssueID = issueID
		}

		analyses = append(analyses, analysis)
	}

	return analyses
}

func truncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}
