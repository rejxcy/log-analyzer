package main

import (
	"bytes"
	"encoding/base64"
	"encoding/json"
	"flag"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"time"

	"log-analyzer/internal/aggregator"
	"log-analyzer/internal/config"
	"log-analyzer/internal/normalizer"
	"log-analyzer/internal/preprocessor"
	"log-analyzer/internal/reporter"
	"log-analyzer/pkg/models"
)

func main() {
	// Only one parameter: time range
	timeRange := flag.String("time", "24h", "Time range for OpenSearch query (e.g., '1h', '24h', '7d')")
	flag.Parse()

	fmt.Println("ğŸš€ å•Ÿå‹•æ—¥èªŒåˆ†æç®¡é“")
	fmt.Println()

	// Load configuration
	cfg, err := config.Load("./configs/config.yaml")
	if err != nil {
		log.Fatalf("âŒ ç„¡æ³•åŠ è¼‰é…ç½®ï¼š%v", err)
	}

	// Step 0: Fetch from OpenSearch with time windows
	fmt.Printf("ğŸ“¡ ç¬¬ 0 æ­¥ï¼šå¾ OpenSearch ç²å–æ—¥èªŒï¼ˆéå» %sï¼‰...\n", *timeRange)
	rawLogs, err := fetchFromOpenSearchWithWindows(cfg, *timeRange)
	if err != nil {
		log.Fatalf("âŒ ç„¡æ³•å¾ OpenSearch ç²å–ï¼š%v", err)
	}

	if len(rawLogs) == 0 {
		fmt.Println("âš ï¸  æŒ‡å®šæ™‚é–“ç¯„åœå…§æ‰¾ä¸åˆ°æ—¥èªŒã€‚")
		fmt.Println("   æç¤ºï¼šå˜—è©¦æ›´é•·çš„æ™‚é–“ç¯„åœï¼ˆä¾‹å¦‚ï¼š-time 48hï¼‰")
		os.Exit(0)
	}

	fmt.Printf("âœ… æˆåŠŸç²å– %d æ¢åŸå§‹æ—¥èªŒ\n", len(rawLogs))

	// Show service distribution from raw logs
	serviceDistribution := make(map[string]int)
	for _, log := range rawLogs {
		serviceName := log.Source.Fields.ServiceName
		if serviceName == "" {
			serviceName = "unknown"
		}
		serviceDistribution[serviceName]++
	}
	fmt.Println("   æœå‹™åˆ†ä½ˆï¼š")
	for service, count := range serviceDistribution {
		fmt.Printf("   - %s: %d æ¢æ—¥èªŒ\n", service, count)
	}
	fmt.Println()

	// Step 1: Preprocess logs
	fmt.Println("ğŸ”„ ç¬¬ 1 æ­¥ï¼šé è™•ç†æ—¥èªŒ...")
	prep := preprocessor.NewLogPreprocessor()
	parsedLogs, err := prep.Process(rawLogs)
	if err != nil {
		log.Fatalf("âŒ ç„¡æ³•é è™•ç†æ—¥èªŒï¼š%v", err)
	}
	fmt.Printf("âœ… æˆåŠŸè§£æ %d æ¢æ—¥èªŒ\n\n", len(parsedLogs))

	// Step 2: Normalize and group by fingerprint
	fmt.Println("ğŸ” ç¬¬ 2 æ­¥ï¼šæ­£è¦åŒ–å’Œåˆ†çµ„éŒ¯èª¤...")
	norm := normalizer.NewLogNormalizer()
	errorGroups, err := norm.Normalize(parsedLogs)
	if err != nil {
		log.Fatalf("âŒ ç„¡æ³•æ­£è¦åŒ–æ—¥èªŒï¼š%v", err)
	}
	normStats := normalizer.GetNormalizationStats(len(parsedLogs), errorGroups)
	fmt.Printf("âœ… åˆ†çµ„ç‚º %d å€‹å”¯ä¸€éŒ¯èª¤æ¨¡å¼ï¼ˆ%.1f%% é‡è¤‡ç‡ï¼‰\n\n",
		len(errorGroups), normStats.DuplicationRate*100)

	// Step 3: Aggregate statistics
	fmt.Println("ğŸ“Š ç¬¬ 3 æ­¥ï¼šèšåˆçµ±è¨ˆè³‡è¨Š...")
	agg := aggregator.NewLogAggregator()
	aggResult, err := agg.Aggregate(errorGroups)
	if err != nil {
		log.Fatalf("âŒ ç„¡æ³•èšåˆï¼š%v", err)
	}
	aggStats := aggregator.GetAggregationStats(aggResult)
	fmt.Printf("âœ… èšåˆå®Œæˆï¼š\n")
	fmt.Printf("   - ç¸½éŒ¯èª¤æ•¸ï¼š%d\n", aggStats.TotalLogs)
	fmt.Printf("   - æœå‹™ç¸½æ•¸ï¼š%d\n", aggStats.TotalServices)
	fmt.Printf("   - å³°å€¼æ™‚æ®µï¼š%02d:00ï¼ˆ%d å€‹éŒ¯èª¤ï¼‰\n", aggStats.PeakHour, aggStats.PeakCount)
	fmt.Printf("   - å¹³å‡å¯†åº¦ï¼š%.2f éŒ¯èª¤/åˆ†é˜\n\n", aggStats.AverageDensity)

	// Step 4: Generate analyses from actual error groups
	fmt.Println("ğŸ” ç¬¬ 4 æ­¥ï¼šåˆ†æéŒ¯èª¤æ¨¡å¼...")
	analyses := createAnalysesFromErrorGroups(errorGroups)
	fmt.Printf("âœ… å¾å¯¦éš›æ•¸æ“šå»ºç«‹äº† %d å€‹åˆ†æçµæœ\n\n", len(analyses))

	// Step 5: Generate reports (one per service)
	fmt.Println("ğŸ“„ ç¬¬ 5 æ­¥ï¼šç‚ºæ¯å€‹æœå‹™ç”Ÿæˆ Markdown å ±å‘Š...")

	// Group analyses by service
	analysesByService := make(map[string][]models.Analysis)
	for _, analysis := range analyses {
		// Find the service for this analysis from errorGroups
		for _, group := range errorGroups {
			if group.Fingerprint[:8] == analysis.ErrorGroupID {
				analysesByService[group.ServiceName] = append(analysesByService[group.ServiceName], analysis)
				break
			}
		}
	}

	rep := reporter.NewMarkdownReporter(cfg.Output.ReportDir)

	// Generate one report per service
	for service, serviceAnalyses := range analysesByService {
		report, err := rep.GeneratePerService(serviceAnalyses, aggResult, service)
		if err != nil {
			fmt.Printf("âŒ ç„¡æ³•ç”Ÿæˆ %s çš„å ±å‘Šï¼š%v\n", service, err)
			continue
		}
		fmt.Printf("âœ… %s å ±å‘Šå·²ç”Ÿæˆï¼š%s\n", service, report.ReportPath)
	}
	fmt.Println()

	// Step 6: Save analysis JSON
	fmt.Println("ğŸ’¾ ç¬¬ 6 æ­¥ï¼šå°‡åˆ†æçµæœä¿å­˜ç‚º JSON...")
	if err := reporter.SaveAnalysisJSON(analyses, aggResult, cfg.Output.ReportDir); err != nil {
		log.Fatalf("âŒ ç„¡æ³•ä¿å­˜åˆ†æ JSONï¼š%v", err)
	}
	fmt.Println("âœ… åˆ†æ JSON å·²ä¿å­˜")

	// Summary
	fmt.Println(strings.Repeat("=", 60))
	fmt.Println("âœ¨ å®Œæ•´ç®¡é“åˆ†ææˆåŠŸå®Œæˆï¼")
	fmt.Println(strings.Repeat("=", 60))
	fmt.Printf("\nğŸ“Š æœ€çµ‚çµ±è¨ˆè³‡è¨Šï¼š\n")
	fmt.Printf("   è¼¸å…¥æ—¥èªŒæ•¸ï¼š%d\n", len(rawLogs))
	fmt.Printf("   è§£ææ—¥èªŒæ•¸ï¼š%d\n", len(parsedLogs))
	fmt.Printf("   éŒ¯èª¤ç¾¤çµ„æ•¸ï¼š%d\n", len(errorGroups))
	fmt.Printf("   å—å½±éŸ¿æœå‹™æ•¸ï¼š%d\n", len(aggResult.ServiceStats))
	fmt.Printf("   è™•ç†æ™‚é–“ï¼š%dms\n\n", aggResult.ProcessingTime.Milliseconds())

	fmt.Printf("ğŸ“ è¼¸å‡ºæª”æ¡ˆï¼š\n")
	fmt.Printf("   å ±å‘Šç›®éŒ„ï¼š%s\n", cfg.Output.ReportDir)
	fmt.Printf("   åˆ†æ JSONï¼š%s/analysis_*.json\n\n", cfg.Output.ReportDir)

	fmt.Println("âœ… æ‚¨ç¾åœ¨å¯ä»¥æŸ¥çœ‹ç”Ÿæˆçš„å ±å‘Šå’Œåˆ†æ JSON æª”æ¡ˆï¼")
}

// fetchFromOpenSearchWithWindows fetches logs with time window splitting
func fetchFromOpenSearchWithWindows(cfg *config.Config, timeRangeStr string) ([]models.RawLog, error) {
	// Parse time range and window size
	duration, err := time.ParseDuration(timeRangeStr)
	if err != nil {
		return nil, fmt.Errorf("invalid time range: %w", err)
	}

	windowDuration, err := time.ParseDuration("30m") // Fixed window size from config
	if err != nil {
		return nil, fmt.Errorf("invalid window size: %w", err)
	}

	endTime := time.Now()

	// Calculate number of windows
	numWindows := int(duration / windowDuration)
	if numWindows == 0 {
		numWindows = 1
	}

	fmt.Printf("   ğŸ“Š è·¨ %d å€‹æ™‚é–“çª—å£ç²å–æ—¥èªŒï¼ˆæ¯å€‹ %.0f åˆ†é˜ï¼‰\n", numWindows, windowDuration.Minutes())
	fmt.Println()

	var allLogs []models.RawLog

	// Fetch data for each window
	for i := 0; i < numWindows; i++ {
		windowEnd := endTime.Add(-time.Duration(i) * windowDuration)
		windowStart := windowEnd.Add(-windowDuration)

		fmt.Printf("   ğŸ• çª—å£ %d/%dï¼š%s åˆ° %s\n", i+1, numWindows,
			windowStart.Format("15:04:05"), windowEnd.Format("15:04:05"))

		logs, err := fetchFromOpenSearchDashboards(cfg, windowStart, windowEnd)
		if err != nil {
			fmt.Printf("      âŒ éŒ¯èª¤ï¼š%v\n", err)
			continue
		}

		fmt.Printf("      âœ… å…± %d æ¢æ—¥èªŒ\n", len(logs))
		allLogs = append(allLogs, logs...)
	}

	fmt.Println()
	return allLogs, nil
}

// fetchFromOpenSearchDashboards fetches logs from a specific time window
func fetchFromOpenSearchDashboards(cfg *config.Config, startTime, endTime time.Time) ([]models.RawLog, error) {
	// Build query
	query := buildDashboardsQuery(startTime, endTime, cfg.Query.Keyword)

	// Make request
	client := &http.Client{Timeout: 30 * time.Second}

	var allLogs []models.RawLog

	for _, index := range cfg.OpenSearch.Indices {
		index = strings.TrimSpace(index)

		body := map[string]interface{}{
			"params": map[string]interface{}{
				"index": index,
				"body":  query,
			},
		}

		jsonBody, err := json.Marshal(body)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal request: %w", err)
		}

		url := fmt.Sprintf("%s/internal/search/opensearch-with-long-numerals", cfg.OpenSearch.URL)
		req, err := http.NewRequest("POST", url, bytes.NewBuffer(jsonBody))
		if err != nil {
			return nil, fmt.Errorf("failed to create request: %w", err)
		}

		// Set headers
		req.Header.Set("Content-Type", "application/json")
		req.Header.Set("Authorization", "Basic "+basicAuth(cfg.OpenSearch.Username, cfg.OpenSearch.Password))
		req.Header.Set("osd-xsrf", "osd-fetch")
		req.Header.Set("osd-version", "3.0.0")

		resp, err := client.Do(req)
		if err != nil {
			fmt.Printf("         [%s] âŒ é€£æ¥å¤±æ•—ï¼š%v\n", index, err)
			continue
		}
		defer resp.Body.Close()

		if resp.StatusCode != http.StatusOK {
			bodyBytes, _ := io.ReadAll(resp.Body)
			fmt.Printf("         [%s] âŒ API è¿”å› %dï¼š%s\n", index, resp.StatusCode, string(bodyBytes))
			continue
		}

		// Parse response
		var response map[string]interface{}
		if err := json.NewDecoder(resp.Body).Decode(&response); err != nil {
			fmt.Printf("         [%s] âŒ è§£æéŸ¿æ‡‰å¤±æ•—ï¼š%v\n", index, err)
			continue
		}

		// Extract hits
		var hitsArray []interface{}
		if rawResp, ok := response["rawResponse"].(map[string]interface{}); ok {
			if hits, ok := rawResp["hits"].(map[string]interface{}); ok {
				if hits, ok := hits["hits"].([]interface{}); ok {
					hitsArray = hits
				}
			}
		}

		// Process hits
		logsFromIndex := 0
		for _, hit := range hitsArray {
			hitMap := hit.(map[string]interface{})

			source, ok := hitMap["_source"].(map[string]interface{})
			if !ok {
				continue
			}

			sourceBytes, _ := json.Marshal(source)
			var openSearchSource models.OpenSearchSource
			if err := json.Unmarshal(sourceBytes, &openSearchSource); err != nil {
				continue
			}

			rawLog := models.RawLog{
				Index:  index,
				ID:     hitMap["_id"].(string),
				Source: openSearchSource,
			}

			allLogs = append(allLogs, rawLog)
			logsFromIndex++
		}

		if logsFromIndex > 0 {
			fmt.Printf("         [%s] âœ… %d æ¢\n", index, logsFromIndex)
		}
	}

	return allLogs, nil
}

// buildDashboardsQuery builds a query for specific time window
func buildDashboardsQuery(startTime, endTime time.Time, keyword string) map[string]interface{} {
	return map[string]interface{}{
		"sort": []map[string]interface{}{
			{
				"@timestamp": map[string]interface{}{
					"order":         "desc",
					"unmapped_type": "boolean",
				},
			},
		},
		"size": 500,
		"_source": map[string]interface{}{
			"excludes": []string{},
		},
		"query": map[string]interface{}{
			"bool": map[string]interface{}{
				"must": []interface{}{},
				"filter": []interface{}{
					map[string]interface{}{
						"multi_match": map[string]interface{}{
							"type":    "phrase",
							"query":   keyword,
							"lenient": true,
						},
					},
					map[string]interface{}{
						"range": map[string]interface{}{
							"@timestamp": map[string]interface{}{
								"gte":    startTime.Format(time.RFC3339),
								"lte":    endTime.Format(time.RFC3339),
								"format": "strict_date_optional_time",
							},
						},
					},
				},
				"should":   []interface{}{},
				"must_not": []interface{}{},
			},
		},
	}
}

// basicAuth creates a basic auth header value
func basicAuth(username, password string) string {
	credentials := fmt.Sprintf("%s:%s", username, password)
	return base64.StdEncoding.EncodeToString([]byte(credentials))
}

// createAnalysesFromErrorGroups creates analysis results from actual error groups
func createAnalysesFromErrorGroups(groups []models.ErrorGroup) []models.Analysis {
	var analyses []models.Analysis
	registry := config.GetRegistry()

	for _, group := range groups {
		severity := models.SeverityLow
		if group.TotalCount >= 50 {
			severity = models.SeverityHigh
		} else if group.TotalCount >= 10 {
			severity = models.SeverityMedium
		}

		// Try to match against known issues
		var isKnown bool
		var issueID string
		matchedIssue := registry.MatchContentAndService(group.NormalizedContent, group.ServiceName)
		if matchedIssue != nil {
			isKnown = true
			issueID = matchedIssue.ID
		}

		analysis := models.Analysis{
			ErrorGroupID: group.Fingerprint[:8],
			IsKnown:      isKnown,
			Severity:     severity,
			Reason:       fmt.Sprintf("éŒ¯èª¤åœ¨æœå‹™ %s ä¸­ç™¼ç”Ÿäº† %d æ¬¡", group.ServiceName, group.TotalCount),
			SuggestedActions: []string{
				fmt.Sprintf("èª¿æŸ¥éŒ¯èª¤æ¨¡å¼ï¼š%s", truncateString(group.NormalizedContent, 60)),
				fmt.Sprintf("æª¢æŸ¥ä¾†è‡ªèª¿ç”¨è€…çš„æ—¥èªŒï¼š%s", group.CallerFile),
				fmt.Sprintf("èˆ‡éƒ¨ç½²æˆ–é…ç½®è®Šæ›´ç›¸é—œè¯"),
			},
		}

		if isKnown {
			analysis.IssueID = issueID
		}

		analyses = append(analyses, analysis)
	}

	return analyses
}

func truncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}
