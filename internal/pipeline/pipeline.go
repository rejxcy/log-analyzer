package pipeline

import (
	"fmt"

	"log-analyzer/internal/aggregator"
	"log-analyzer/internal/config"
	"log-analyzer/internal/fetcher"
	"log-analyzer/internal/interfaces"
	"log-analyzer/internal/normalizer"
	"log-analyzer/internal/preprocessor"
	"log-analyzer/internal/reporter"
	"log-analyzer/pkg/models"
)

// Pipeline orchestrates the entire log analysis workflow
type Pipeline struct {
	fetcher      *fetcher.Fetcher
	preprocessor *preprocessor.LogPreprocessor
	normalizer   *normalizer.LogNormalizer
	aggregator   *aggregator.LogAggregator
	reporter     *reporter.MarkdownReporter
	config       *config.Config
}

// NewPipeline creates a new pipeline
func NewPipeline(cfg *config.Config) *Pipeline {
	return &Pipeline{
		fetcher:      fetcher.NewFetcher(cfg),
		preprocessor: preprocessor.NewLogPreprocessor(),
		normalizer:   normalizer.NewLogNormalizer(),
		aggregator:   aggregator.NewLogAggregator(),
		reporter:     reporter.NewMarkdownReporter(cfg.Output.ReportDir),
		config:       cfg,
	}
}

// PipelineResult represents the result of running the pipeline
type PipelineResult struct {
	RawLogs           []models.RawLog
	ParsedLogs        []models.ParsedLog
	ErrorGroups       []models.ErrorGroup
	Analyses          []models.Analysis
	AggregationResult *interfaces.AggregationResult
	Reports           map[string]*models.Report
}

// Run executes the entire pipeline
func (p *Pipeline) Run(timeRangeStr string) (*PipelineResult, error) {
	result := &PipelineResult{
		Reports: make(map[string]*models.Report),
	}

	// Step 0: Fetch from OpenSearch
	fmt.Printf("ğŸ“¡ ç¬¬ 0 æ­¥ï¼šå¾ OpenSearch ç²å–æ—¥èªŒï¼ˆéå» %sï¼‰...\n", timeRangeStr)
	rawLogs, err := p.fetcher.FetchWithTimeWindows(timeRangeStr)
	if err != nil {
		return nil, fmt.Errorf("fetching failed: %w", err)
	}

	if len(rawLogs) == 0 {
		fmt.Println("âš ï¸  æŒ‡å®šæ™‚é–“ç¯„åœå…§æ‰¾ä¸åˆ°æ—¥èªŒã€‚")
		fmt.Println("   æç¤ºï¼šå˜—è©¦æ›´é•·çš„æ™‚é–“ç¯„åœï¼ˆä¾‹å¦‚ï¼š-time 48hï¼‰")
		return result, nil
	}

	fmt.Printf("âœ… æˆåŠŸç²å– %d æ¢åŸå§‹æ—¥èªŒ\n", len(rawLogs))
	result.RawLogs = rawLogs

	// Show service distribution
	p.printServiceDistribution(rawLogs)

	// Step 1: Preprocess
	fmt.Println("ğŸ”„ ç¬¬ 1 æ­¥ï¼šé è™•ç†æ—¥èªŒ...")
	parsedLogs, err := p.preprocessor.Process(rawLogs)
	if err != nil {
		return nil, fmt.Errorf("preprocessing failed: %w", err)
	}
	fmt.Printf("âœ… æˆåŠŸè§£æ %d æ¢æ—¥èªŒ\n\n", len(parsedLogs))
	result.ParsedLogs = parsedLogs

	// Step 2: Normalize
	fmt.Println("ğŸ” ç¬¬ 2 æ­¥ï¼šæ­£è¦åŒ–å’Œåˆ†çµ„éŒ¯èª¤...")
	errorGroups, err := p.normalizer.Normalize(parsedLogs)
	if err != nil {
		return nil, fmt.Errorf("normalization failed: %w", err)
	}
	normStats := normalizer.GetNormalizationStats(len(parsedLogs), errorGroups)
	fmt.Printf("âœ… åˆ†çµ„ç‚º %d å€‹å”¯ä¸€éŒ¯èª¤æ¨¡å¼ï¼ˆ%.1f%% é‡è¤‡ç‡ï¼‰\n\n",
		len(errorGroups), normStats.DuplicationRate*100)
	result.ErrorGroups = errorGroups

	// Step 3: Aggregate
	fmt.Println("ğŸ“Š ç¬¬ 3 æ­¥ï¼šèšåˆçµ±è¨ˆè³‡è¨Š...")
	aggResult, err := p.aggregator.Aggregate(errorGroups)
	if err != nil {
		return nil, fmt.Errorf("aggregation failed: %w", err)
	}
	aggStats := aggregator.GetAggregationStats(aggResult)
	fmt.Printf("âœ… èšåˆå®Œæˆï¼š\n")
	fmt.Printf("   - ç¸½éŒ¯èª¤æ•¸ï¼š%d\n", aggStats.TotalLogs)
	fmt.Printf("   - æœå‹™ç¸½æ•¸ï¼š%d\n", aggStats.TotalServices)
	fmt.Printf("   - å³°å€¼æ™‚æ®µï¼š%02d:00ï¼ˆ%d å€‹éŒ¯èª¤ï¼‰\n", aggStats.PeakHour, aggStats.PeakCount)
	fmt.Printf("   - å¹³å‡å¯†åº¦ï¼š%.2f éŒ¯èª¤/åˆ†é˜\n\n", aggStats.AverageDensity)
	result.AggregationResult = aggResult

	// Step 4: Analyze
	fmt.Println("ğŸ” ç¬¬ 4 æ­¥ï¼šåˆ†æéŒ¯èª¤æ¨¡å¼...")
	analyses := p.createAnalysesFromErrorGroups(errorGroups)
	fmt.Printf("âœ… å¾å¯¦éš›æ•¸æ“šå»ºç«‹äº† %d å€‹åˆ†æçµæœ\n\n", len(analyses))
	result.Analyses = analyses

	// Step 5: Generate reports (one per service)
	fmt.Println("ğŸ“„ ç¬¬ 5 æ­¥ï¼šç‚ºæ¯å€‹æœå‹™ç”Ÿæˆ Markdown å ±å‘Š...")
	if err := p.generatePerServiceReports(analyses, errorGroups, aggResult, result); err != nil {
		return nil, fmt.Errorf("report generation failed: %w", err)
	}
	fmt.Println()

	// Step 6: Save JSON
	fmt.Println("ğŸ’¾ ç¬¬ 6 æ­¥ï¼šå°‡åˆ†æçµæœä¿å­˜ç‚º JSON...")
	if err := reporter.SaveAnalysisJSON(analyses, aggResult, p.config.Output.ReportDir); err != nil {
		return nil, fmt.Errorf("saving JSON failed: %w", err)
	}
	fmt.Println("âœ… åˆ†æ JSON å·²ä¿å­˜")

	return result, nil
}

// printServiceDistribution prints service distribution from raw logs
func (p *Pipeline) printServiceDistribution(rawLogs []models.RawLog) {
	serviceDistribution := make(map[string]int)
	for _, log := range rawLogs {
		serviceName := log.Source.Fields.ServiceName
		if serviceName == "" {
			serviceName = "unknown"
		}
		serviceDistribution[serviceName]++
	}
	fmt.Println("   æœå‹™åˆ†ä½ˆï¼š")
	for service, count := range serviceDistribution {
		fmt.Printf("   - %s: %d æ¢æ—¥èªŒ\n", service, count)
	}
	fmt.Println()
}

// createAnalysesFromErrorGroups creates analysis results from error groups
func (p *Pipeline) createAnalysesFromErrorGroups(groups []models.ErrorGroup) []models.Analysis {
	var analyses []models.Analysis
	registry := config.GetRegistry()

	for _, group := range groups {
		severity := models.SeverityLow
		if group.TotalCount >= 50 {
			severity = models.SeverityHigh
		} else if group.TotalCount >= 10 {
			severity = models.SeverityMedium
		}

		// Try to match against known issues
		var isKnown bool
		var issueID string
		matchedIssue := registry.MatchContentAndService(group.NormalizedContent, group.ServiceName)
		if matchedIssue != nil {
			isKnown = true
			issueID = matchedIssue.ID
		}

		analysis := models.Analysis{
			ErrorGroupID: group.Fingerprint[:8],
			IsKnown:      isKnown,
			Severity:     severity,
			Reason:       fmt.Sprintf("éŒ¯èª¤åœ¨æœå‹™ %s ä¸­ç™¼ç”Ÿäº† %d æ¬¡", group.ServiceName, group.TotalCount),
			SuggestedActions: []string{
				fmt.Sprintf("èª¿æŸ¥éŒ¯èª¤æ¨¡å¼ï¼š%s", truncateString(group.NormalizedContent, 60)),
				fmt.Sprintf("æª¢æŸ¥ä¾†è‡ªèª¿ç”¨è€…çš„æ—¥èªŒï¼š%s", group.CallerFile),
				fmt.Sprintf("èˆ‡éƒ¨ç½²æˆ–é…ç½®è®Šæ›´ç›¸é—œè¯"),
			},
		}

		if isKnown {
			analysis.IssueID = issueID
		}

		analyses = append(analyses, analysis)
	}

	return analyses
}

// generatePerServiceReports generates reports for each service
func (p *Pipeline) generatePerServiceReports(analyses []models.Analysis, errorGroups []models.ErrorGroup,
	aggResult *interfaces.AggregationResult, result *PipelineResult) error {

	// Group analyses by service
	analysesByService := make(map[string][]models.Analysis)
	for _, analysis := range analyses {
		// Find the service for this analysis from errorGroups
		for _, group := range errorGroups {
			if group.Fingerprint[:8] == analysis.ErrorGroupID {
				analysesByService[group.ServiceName] = append(analysesByService[group.ServiceName], analysis)
				break
			}
		}
	}

	// Generate one report per service
	for service, serviceAnalyses := range analysesByService {
		report, err := p.reporter.GeneratePerService(serviceAnalyses, aggResult, service)
		if err != nil {
			fmt.Printf("âŒ ç„¡æ³•ç”Ÿæˆ %s çš„å ±å‘Šï¼š%v\n", service, err)
			continue
		}
		fmt.Printf("âœ… %s å ±å‘Šå·²ç”Ÿæˆï¼š%s\n", service, report.ReportPath)
		result.Reports[service] = report
	}

	return nil
}

// truncateString truncates a string to the specified length
func truncateString(s string, maxLen int) string {
	if len(s) <= maxLen {
		return s
	}
	return s[:maxLen] + "..."
}
